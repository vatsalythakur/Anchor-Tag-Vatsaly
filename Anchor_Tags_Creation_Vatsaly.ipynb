{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bebd4386-a214-4678-bd41-784d61edd0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>standardize_type_updated</th>\n",
       "      <th>normalised_name</th>\n",
       "      <th>confidence</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>466708619_pink</td>\n",
       "      <td>shop the vibe</td>\n",
       "      <td>magenta kurta set</td>\n",
       "      <td>0.98</td>\n",
       "      <td>Women - Kurta Suit Sets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>466708619_pink</td>\n",
       "      <td>shop the vibe</td>\n",
       "      <td>gold embroidered kurta</td>\n",
       "      <td>0.97</td>\n",
       "      <td>Women - Kurta Suit Sets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>466708619_pink</td>\n",
       "      <td>shop the vibe</td>\n",
       "      <td>three quarter sleeve kurta</td>\n",
       "      <td>0.96</td>\n",
       "      <td>Women - Kurta Suit Sets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>466708619_pink</td>\n",
       "      <td>shop the vibe</td>\n",
       "      <td>traditional festive wear</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Women - Kurta Suit Sets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>466708619_pink</td>\n",
       "      <td>shop the vibe</td>\n",
       "      <td>ethnic kurta suit set</td>\n",
       "      <td>0.94</td>\n",
       "      <td>Women - Kurta Suit Sets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id standardize_type_updated             normalised_name  \\\n",
       "0  466708619_pink            shop the vibe           magenta kurta set   \n",
       "1  466708619_pink            shop the vibe      gold embroidered kurta   \n",
       "2  466708619_pink            shop the vibe  three quarter sleeve kurta   \n",
       "3  466708619_pink            shop the vibe    traditional festive wear   \n",
       "4  466708619_pink            shop the vibe       ethnic kurta suit set   \n",
       "\n",
       "   confidence                 category  \n",
       "0        0.98  Women - Kurta Suit Sets  \n",
       "1        0.97  Women - Kurta Suit Sets  \n",
       "2        0.96  Women - Kurta Suit Sets  \n",
       "3        0.95  Women - Kurta Suit Sets  \n",
       "4        0.94  Women - Kurta Suit Sets  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"women_kurta_suit_sets_tags.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8154a738-8c50-413d-94b9-4f9277884ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5010883, 5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66530b0e-52e0-4880-84c0-7304d8b942dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50923"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['normalised_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "634d63fc-faf8-406e-b6a3-f0dfadae227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tags = (\n",
    "    df[\"normalised_name\"]\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d2c9df4-063b-4f4a-85cc-064013243f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50913,\n",
       " ['magenta kurta set',\n",
       "  'gold embroidered kurta',\n",
       "  'three quarter sleeve kurta',\n",
       "  'traditional festive wear',\n",
       "  'ethnic kurta suit set',\n",
       "  'knee length kurta',\n",
       "  'side slit kurta',\n",
       "  'gold trim ethnic wear',\n",
       "  'straight pant kurta set',\n",
       "  'summer ethnic outfit'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_tags), raw_tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9fafe111-00d3-4b86-a215-cbd33634055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_phrase(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)   # normalize spaces\n",
    "    return text\n",
    "\n",
    "normalized_tags = [normalize_phrase(t) for t in raw_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d50af483-f76e-4ed0-b68d-5e472134047d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20450            solid base with embroidery\n",
       "45534    bottomwear fabric chanderi jaquard\n",
       "18607             rust orange festive kurta\n",
       "23989              floral and striped combo\n",
       "37199          printed churidar and dupatta\n",
       "40710               geometric ethnic border\n",
       "23101            monochromatic accent kurta\n",
       "32825                        pin tuck kurta\n",
       "33581              red cuff detailing kurta\n",
       "16549             traditional elegance wear\n",
       "dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(normalized_tags).sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c95a3c5-8775-4804-ab1c-955df9395371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf26fba8-960c-44ac-8215-ea30d7e19c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from rapidfuzz import fuzz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "57679b73-3cf4-46f1-b886-9447afc2e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49408\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"/data/similars/ajio/fashion-clip\")\n",
    "print(tok.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "691b3433-036b-41ac-8823-e5325522a3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "\n",
    "# ---- Load model (once) ----\n",
    "processor = AutoProcessor.from_pretrained(\"/data/similars/ajio/fashion-clip\")\n",
    "model = AutoModelForZeroShotImageClassification.from_pretrained(\n",
    "    \"/data/similars/ajio/fashion-clip\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1311591b-52c4-47b1-be0d-1e83031271be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Text embedding function (FashionCLIP equivalent) ----\n",
    "def encode_text_fashionclip(texts, batch_size=32):\n",
    "    all_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "            inputs = processor(\n",
    "                text=batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            # FashionCLIP exposes text features like CLIP\n",
    "            text_features = model.get_text_features(**inputs)\n",
    "\n",
    "            # Normalize (important for cosine similarity)\n",
    "            text_features = torch.nn.functional.normalize(\n",
    "                text_features, dim=1\n",
    "            )\n",
    "\n",
    "            all_embeddings.append(text_features.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "86e570f7-b252-4496-8cad-f82a3fdc4194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# ---- EXACT SAME variable name as before ----\n",
    "text_embeddings = encode_text_fashionclip(\n",
    "    normalized_tags,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# (Optional safety â€“ already normalized above)\n",
    "text_embeddings = text_embeddings / np.linalg.norm(\n",
    "    text_embeddings,\n",
    "    axis=1,\n",
    "    keepdims=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a62b7853-a42c-4547-b65f-4ad9aba50ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import numpy as np\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=2,\n",
    "        min_samples=1,\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method=\"eom\",\n",
    "        gen_min_span_tree=True\n",
    "    )\n",
    "\n",
    "cluster_labels = clusterer.fit_predict(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2fa20c39-be8b-482a-9bbf-8ebeb57fabae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ HDBSCAN clusters (excluding noise): 11714\n",
      "ðŸ”¹ Noise points: 14443\n"
     ]
    }
   ],
   "source": [
    "n_clusters_hdbscan = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = np.sum(cluster_labels == -1)\n",
    "\n",
    "print(f\"ðŸ”¹ HDBSCAN clusters (excluding noise): {n_clusters_hdbscan}\")\n",
    "print(f\"ðŸ”¹ Noise points: {n_noise}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "502dc0ec-8515-45a9-8dda-5408908a3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert noise points (-1) into unique cluster IDs\n",
    "if n_noise > 0:\n",
    "    max_cluster_id = cluster_labels[cluster_labels != -1].max() if n_clusters_hdbscan > 0 else -1\n",
    "    noise_indices = np.where(cluster_labels == -1)[0]\n",
    "\n",
    "    for i, idx in enumerate(noise_indices):\n",
    "        cluster_labels[idx] = max_cluster_id + 1 + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8cd8ea43-7d5f-4fd9-9b9c-97c417d6416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final total clusters (including singletons): 26157\n"
     ]
    }
   ],
   "source": [
    "unique_clusters = np.unique(cluster_labels)\n",
    "\n",
    "print(f\"âœ… Final total clusters (including singletons): {len(unique_clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3a4fca4d-f324-4403-8aca-002d4db4cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_part_a(embeddings, labels):\n",
    "    unique_clusters = np.unique(labels)\n",
    "\n",
    "    intra_sims = []\n",
    "    centroids = []\n",
    "\n",
    "    # ---- Intra-cluster similarity ----\n",
    "    for cid in unique_clusters:\n",
    "        mask = labels == cid\n",
    "        cluster_emb = embeddings[mask]\n",
    "\n",
    "        if len(cluster_emb) > 1:\n",
    "            sim_matrix = cosine_similarity(cluster_emb)\n",
    "            avg_sim = (sim_matrix.sum() - len(cluster_emb)) / (\n",
    "                len(cluster_emb) * (len(cluster_emb) - 1)\n",
    "            )\n",
    "            intra_sims.append(avg_sim)\n",
    "\n",
    "        # centroid for inter-cluster\n",
    "        centroids.append(cluster_emb.mean(axis=0))\n",
    "\n",
    "    intra_avg = np.mean(intra_sims)\n",
    "\n",
    "    # ---- Inter-cluster similarity (centroids) ----\n",
    "    centroid_matrix = cosine_similarity(np.vstack(centroids))\n",
    "    np.fill_diagonal(centroid_matrix, np.nan)\n",
    "    inter_avg = np.nanmean(centroid_matrix)\n",
    "\n",
    "    compactness = (intra_avg - inter_avg) / max(intra_avg + inter_avg, 1e-6)\n",
    "\n",
    "    return {\n",
    "        \"intra_cluster_similarity\": intra_avg,\n",
    "        \"inter_cluster_similarity\": inter_avg,\n",
    "        \"compactness_score\": compactness\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "142d6905-47f5-4b01-a9ec-61a4de90bb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Part A Metrics\n",
      "intra_cluster_similarity: 0.9635\n",
      "inter_cluster_similarity: 0.5211\n",
      "compactness_score: 0.2980\n"
     ]
    }
   ],
   "source": [
    "part_a_metrics = evaluate_part_a(\n",
    "    embeddings=text_embeddings,\n",
    "    labels=cluster_labels\n",
    ")\n",
    "\n",
    "print(\"ðŸ”¹ Part A Metrics\")\n",
    "for k, v in part_a_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "736ca652-bebd-432b-8f7f-86b3ba44b011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 'clusters_complete_test_overview.csv' with all clusters!\n",
      "Total clusters: 26157\n",
      "Total tags: 50913\n"
     ]
    }
   ],
   "source": [
    "# Build the clusters DataFrame (if not already done)\n",
    "clusters_df = pd.DataFrame({\n",
    "    \"tag\": normalized_tags,\n",
    "    \"cluster\": cluster_labels\n",
    "})\n",
    "\n",
    "# Add cluster size for sorting\n",
    "cluster_sizes = clusters_df.groupby(\"cluster\").size().reset_index(name=\"cluster_size\")\n",
    "clusters_df = clusters_df.merge(cluster_sizes, on=\"cluster\")\n",
    "\n",
    "# Sort by cluster_id and tag for readability\n",
    "clusters_df = clusters_df.sort_values([\"cluster\", \"tag\"])\n",
    "\n",
    "# Export to CSV (downloadable from Colab)\n",
    "clusters_df.to_csv(\"subset_clusters_formed_HDBSCAN_modify.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Saved 'clusters_complete_test_overview.csv' with all clusters!\")\n",
    "print(f\"Total clusters: {clusters_df['cluster'].nunique()}\")\n",
    "print(f\"Total tags: {len(clusters_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "973b12f2-b8f0-48ad-bd7c-3435466638d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "def pick_fuzzy_canonical(phrases):\n",
    "    if len(phrases) == 1:\n",
    "        return phrases[0]\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for p in phrases:\n",
    "        total = 0\n",
    "        for q in phrases:\n",
    "            if p != q:\n",
    "                total += fuzz.token_set_ratio(p, q)\n",
    "        scores[p] = total / (len(phrases) - 1)\n",
    "\n",
    "    # phrase with highest average similarity\n",
    "    return max(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4a7c158d-4c57-44c3-98f1-b851bee78aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_canonical = {}\n",
    "\n",
    "for cid, group in clusters_df.groupby(\"cluster\"):\n",
    "    phrases = group[\"tag\"].tolist()\n",
    "    canonical = pick_fuzzy_canonical(phrases)\n",
    "    cluster_to_canonical[cid] = canonical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae8866ef-b2f6-4ae3-b373-cbbb4f21e432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved fuzzy_canonical_inspection.csv\n",
      "Total clusters: 26157\n",
      "Total canonical tags: 26157\n"
     ]
    }
   ],
   "source": [
    "# Attach fuzzy canonical tag\n",
    "clusters_df[\"canonical_tag\"] = clusters_df[\"cluster\"].map(cluster_to_canonical)\n",
    "\n",
    "# Sort for human readability\n",
    "inspection_df = clusters_df[\n",
    "    [\"canonical_tag\", \"cluster\", \"cluster_size\", \"tag\"]\n",
    "].sort_values(\n",
    "    by=[\"canonical_tag\", \"cluster\", \"tag\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "inspection_df.to_csv(\n",
    "    \"fuzzy_canonical_inspection_HDBSCAN_modify.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Saved fuzzy_canonical_inspection.csv\")\n",
    "print(f\"Total clusters: {inspection_df['cluster'].nunique()}\")\n",
    "print(f\"Total canonical tags: {inspection_df['canonical_tag'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "01f9022a-c0bd-4848-916f-ddd871cf0c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26157"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canonical_names = list(cluster_to_canonical.values())\n",
    "canonical_names = list(dict.fromkeys(canonical_names))  # unique, preserve order\n",
    "\n",
    "len(canonical_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "206fedbf-635f-4ebe-9d27-0e2e2b541933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Encode canonical cluster names (EXACTLY like before) ----\n",
    "canonical_embeddings = encode_text_fashionclip(\n",
    "    canonical_names,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# (Optional safety â€“ already normalized inside encoder)\n",
    "canonical_embeddings = canonical_embeddings / np.linalg.norm(\n",
    "    canonical_embeddings,\n",
    "    axis=1,\n",
    "    keepdims=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0d132079-48de-4ecb-a636-a9ebfb958741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cluster_sizes = Counter(clusters_df[\"cluster\"])\n",
    "\n",
    "canonical_cluster_sizes = {\n",
    "    cluster_to_canonical[cid]: cluster_sizes[cid]\n",
    "    for cid in cluster_to_canonical\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4b6929d3-83b8-4458-8711-5caf81665a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pastel kurta', 47),\n",
       " ('756 . 0', 46),\n",
       " ('button placket', 37),\n",
       " ('ikat kurta', 34),\n",
       " ('tie dye kurta', 32),\n",
       " ('59 . 5', 31),\n",
       " ('handwoven kurta', 29),\n",
       " ('light lavender hue', 26),\n",
       " ('ikat design', 26),\n",
       " ('block print', 26)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(canonical_cluster_sizes.items(), key=lambda x: -x[1])[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f749047b-e6dc-48af-bf26-5bba2d7e4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "STRONG_EMB_THRESHOLD = 0.95\n",
    "WEAK_EMB_THRESHOLD = 0.85\n",
    "FUZZY_THRESHOLD = 80\n",
    "ANCHOR_CLUSTER_SIZE = 4\n",
    "\n",
    "vocab_names = []\n",
    "vocab_embeddings = []\n",
    "final_canonical_map = {}\n",
    "\n",
    "# -------------------------------\n",
    "# PHASE 1: Anchor strong canonicals\n",
    "# -------------------------------\n",
    "for name, emb in zip(canonical_names, canonical_embeddings):\n",
    "    if canonical_cluster_sizes.get(name, 0) >= ANCHOR_CLUSTER_SIZE:\n",
    "        vocab_names.append(name)\n",
    "        vocab_embeddings.append(emb)\n",
    "        final_canonical_map[name] = name\n",
    "\n",
    "# -------------------------------\n",
    "# PHASE 2: Greedy registry for rest\n",
    "# -------------------------------\n",
    "for name, emb in zip(canonical_names, canonical_embeddings):\n",
    "\n",
    "    # already anchored â†’ skip\n",
    "    if name in final_canonical_map:\n",
    "        continue\n",
    "\n",
    "    assigned = False\n",
    "\n",
    "    if vocab_embeddings:\n",
    "        sims = np.dot(vocab_embeddings, emb)\n",
    "        best_idx = np.argmax(sims)\n",
    "        best_sim = sims[best_idx]\n",
    "\n",
    "        fuzzy_sim = fuzz.token_set_ratio(\n",
    "            name,\n",
    "            vocab_names[best_idx]\n",
    "        )\n",
    "\n",
    "        if best_sim >= STRONG_EMB_THRESHOLD:\n",
    "            final_canonical_map[name] = vocab_names[best_idx]\n",
    "            assigned = True\n",
    "\n",
    "        elif best_sim >= WEAK_EMB_THRESHOLD and fuzzy_sim >= FUZZY_THRESHOLD:\n",
    "            final_canonical_map[name] = vocab_names[best_idx]\n",
    "            assigned = True\n",
    "\n",
    "    if not assigned:\n",
    "        vocab_names.append(name)\n",
    "        vocab_embeddings.append(emb)\n",
    "        final_canonical_map[name] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8da01e45-9464-4e34-a279-ce51a3ed909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_final_canonical = {\n",
    "    cid: final_canonical_map[canon]\n",
    "    for cid, canon in cluster_to_canonical.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "90120ebb-a2c1-48f9-8d0c-fa959858be11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved final_canonical_registry.csv\n",
      "Total final canonical tags: 11240\n"
     ]
    }
   ],
   "source": [
    "clusters_df[\"final_canonical_tag\"] = clusters_df[\"cluster\"].map(cluster_to_final_canonical)\n",
    "\n",
    "final_df = clusters_df[\n",
    "    [\"final_canonical_tag\", \"canonical_tag\", \"cluster\", \"tag\"]\n",
    "].sort_values(\n",
    "    [\"final_canonical_tag\", \"cluster\", \"tag\"]\n",
    ")\n",
    "\n",
    "final_df.to_csv(\"final_canonical_registry__HDBSCAN_Anchor_thres_4_emb_thres_0.95_fuzzy_85.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Saved final_canonical_registry.csv\")\n",
    "print(\"Total final canonical tags:\", final_df[\"final_canonical_tag\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fb93140d-1c0d-4c76-942e-7d3db5d82ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assign numeric labels to final canonicals\n",
    "final_canonicals = list(dict.fromkeys(final_canonical_map.values()))\n",
    "final_canonical_to_id = {\n",
    "    name: idx for idx, name in enumerate(final_canonicals)\n",
    "}\n",
    "\n",
    "part_b_labels = np.array([\n",
    "    final_canonical_to_id[final_canonical_map[name]]\n",
    "    for name in canonical_names\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eb382ac4-82a8-46fc-8207-f3defea7bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def l2_normalize(X):\n",
    "    return X / np.linalg.norm(X, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2c6d5ec0-5efb-4831-ad85-212fdbbb9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_intra_similarity(SIM, labels):\n",
    "    intra_vals = []\n",
    "\n",
    "    for g in np.unique(labels):\n",
    "        if g == -1:\n",
    "            continue\n",
    "\n",
    "        idx = np.where(labels == g)[0]\n",
    "        if len(idx) < 2:\n",
    "            continue\n",
    "\n",
    "        sub = SIM[np.ix_(idx, idx)]\n",
    "        n = len(idx)\n",
    "\n",
    "        # remove diagonal (self similarity = 1)\n",
    "        avg = (sub.sum() - n) / (n * (n - 1))\n",
    "        intra_vals.append(avg)\n",
    "\n",
    "    return np.mean(intra_vals) if intra_vals else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ca42b93b-dbf1-4f91-958a-8befefd88807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(E, labels):\n",
    "    centroids = {}\n",
    "    for g in np.unique(labels):\n",
    "        if g == -1:\n",
    "            continue\n",
    "        centroids[g] = E[labels == g].mean(axis=0)\n",
    "    \n",
    "    # normalize centroids\n",
    "    for g in centroids:\n",
    "        centroids[g] /= np.linalg.norm(centroids[g])\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "83dad4aa-a1bb-4f90-b151-c3d2b0560c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_inter_similarity(centroids):\n",
    "    keys = list(centroids.keys())\n",
    "    C = np.vstack([centroids[k] for k in keys])\n",
    "\n",
    "    SIM_C = C @ C.T\n",
    "    n = len(keys)\n",
    "\n",
    "    # exclude diagonal\n",
    "    return (SIM_C.sum() - n) / (n * (n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2ebe37d3-864e-4de8-81ec-c66b5903fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_part_b_fast(embeddings, labels):\n",
    "    E = l2_normalize(embeddings.astype(np.float64))\n",
    "    SIM = E @ E.T\n",
    "\n",
    "    intra = fast_intra_similarity(SIM, labels)\n",
    "    centroids = compute_centroids(E, labels)\n",
    "    inter = fast_inter_similarity(centroids)\n",
    "\n",
    "    compactness = (intra - inter) / max(intra + inter, 1e-6)\n",
    "\n",
    "    return {\n",
    "        \"intra_cluster_similarity\": intra,\n",
    "        \"inter_cluster_dissimilarity\": inter,\n",
    "        \"compactness_score\": compactness\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "76302702-196a-40d7-adad-f11b2a3754e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_part_b_fast(\n",
    "    canonical_embeddings,\n",
    "    part_b_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "325d02b6-1b4e-4ef4-8eec-00968889828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intra-cluster similarity : 0.9218\n",
      "Inter-cluster similarity : 0.5026\n",
      "Compactness score        : 0.2943\n"
     ]
    }
   ],
   "source": [
    "print(f\"Intra-cluster similarity : {results['intra_cluster_similarity']:.4f}\")\n",
    "print(f\"Inter-cluster similarity : {results['inter_cluster_dissimilarity']:.4f}\")\n",
    "print(f\"Compactness score        : {results['compactness_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2d63a5e0-0e30-4695-bb1d-a0f64879db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_df = pd.read_csv(\n",
    "    \"final_canonical_registry__HDBSCAN_Anchor_thres_4_emb_thres_0.95_fuzzy_85.csv\"\n",
    ")\n",
    "\n",
    "# Count raw tags per final canonical\n",
    "cluster_sizes = (\n",
    "    final_df\n",
    "    .groupby(\"final_canonical_tag\")\n",
    "    .size()\n",
    "    .reset_index(name=\"cluster_size\")\n",
    ")\n",
    "\n",
    "# Sort largest â†’ smallest\n",
    "cluster_sizes = cluster_sizes.sort_values(\n",
    "    \"cluster_size\", ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Cumulative raw tag count\n",
    "cluster_sizes[\"cumulative_raw_tags\"] = cluster_sizes[\"cluster_size\"].cumsum()\n",
    "\n",
    "# Cumulative percentage\n",
    "total_tags = cluster_sizes[\"cluster_size\"].sum()\n",
    "cluster_sizes[\"cumulative_pct\"] = (\n",
    "    cluster_sizes[\"cumulative_raw_tags\"] / total_tags\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "searchengine_nlp",
   "language": "python",
   "name": "searchengine_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
